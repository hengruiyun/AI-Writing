# LLM API - ç»Ÿä¸€å¤§è¯­è¨€æ¨¡å‹æ¥å£

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://badge.fury.io/py/llm-api.svg)](https://badge.fury.io/py/llm-api)

ä¸€ä¸ªç»Ÿä¸€çš„APIæ¥å£ï¼Œæ”¯æŒå¤šä¸ªå¤§è¯­è¨€æ¨¡å‹æä¾›å•†ï¼ŒåŒ…æ‹¬OpenAIã€Anthropicã€Googleã€Groqã€DeepSeekã€Ollamaå’ŒLM Studioã€‚

[English](README_EN.md) | ä¸­æ–‡

## åŠŸèƒ½ç‰¹æ€§

- ğŸ”„ **ç»Ÿä¸€æ¥å£**: ä¸€ä¸ªAPIè°ƒç”¨æ‰€æœ‰ä¸»æµLLMæä¾›å•†
- ğŸ¯ **ç»“æ„åŒ–è¾“å‡º**: æ”¯æŒPydanticæ¨¡å‹çš„ç»“æ„åŒ–å“åº”
- ğŸ”§ **è‡ªåŠ¨é‡è¯•**: å†…ç½®é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
- ğŸ  **æœ¬åœ°æ¨¡å‹**: å®Œæ•´çš„Ollamaå’ŒLM Studioæ”¯æŒï¼ŒåŒ…å«æ¨¡å‹ç®¡ç†
- âš¡ **ç¼“å­˜æœºåˆ¶**: æ¨¡å‹å®ä¾‹ç¼“å­˜ï¼Œæå‡æ€§èƒ½
- ğŸ›¡ï¸ **ç±»å‹å®‰å…¨**: å®Œæ•´çš„ç±»å‹æç¤ºæ”¯æŒ
- ğŸŒ **å¤šè¯­è¨€**: æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡
- ğŸ’¬ **Webç•Œé¢**: åŸºäºStreamlitçš„èŠå¤©Webç•Œé¢ï¼Œæ”¯æŒæ‰€æœ‰æä¾›å•†

## æ”¯æŒçš„æä¾›å•†

| æä¾›å•† | ç¤ºä¾‹æ¨¡å‹ | ç¯å¢ƒå˜é‡ |
|--------|----------|----------|
| OpenAI | gpt-4o, gpt-4.1 | `OPENAI_API_KEY` |
| Anthropic | claude-3-5-haiku-latest | `ANTHROPIC_API_KEY` |
| Google | gemini-2.5-flash-preview | `GOOGLE_API_KEY` |
| Groq | llama-4-scout-17b | `GROQ_API_KEY` |
| DeepSeek | deepseek-reasoner | `DEEPSEEK_API_KEY` |
| Ollama | llama3.1:latest | æ— éœ€APIå¯†é’¥ |
| LM Studio | llama3.1:latest | æ— éœ€APIå¯†é’¥ |

## å®‰è£…

### ä»PyPIå®‰è£…ï¼ˆæ¨èï¼‰

```bash
pip install llm-api
```

### ä»æºç å®‰è£…

```bash
git clone https://github.com/ai-hedge-fund/llm-api.git
cd llm-api
pip install -e .
```

### å¼€å‘ç¯å¢ƒå®‰è£…

```bash
git clone https://github.com/ai-hedge-fund/llm-api.git
cd llm-api
pip install -e ".[dev]"
```

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬èŠå¤©

```python
from llm_api import LLMClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = LLMClient()

# ç®€å•èŠå¤©
response = client.chat("ä½ å¥½ï¼Œä¸–ç•Œï¼")
print(response)

# æŒ‡å®šæ¨¡å‹å’Œæä¾›å•†
response = client.chat(
    "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ ",
    model="gpt-4o",
    provider="OpenAI"
)
print(response)
```

### ç»“æ„åŒ–è¾“å‡º

```python
from pydantic import BaseModel
from llm_api import LLMClient

class AnalysisResult(BaseModel):
    sentiment: str
    confidence: float
    summary: str

client = LLMClient()

result = client.chat_with_structured_output(
    "åˆ†æè¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿï¼š'ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…å¾ˆæ„‰å¿«ï¼'",
    pydantic_model=AnalysisResult,
    model="gpt-4o"
)

print(f"æƒ…æ„Ÿ: {result.sentiment}")
print(f"ç½®ä¿¡åº¦: {result.confidence}")
print(f"æ‘˜è¦: {result.summary}")
```

## Chat Web UI

LLM-API æä¾›äº†ä¸€ä¸ªåŸºäº Streamlit çš„ç°ä»£åŒ–èŠå¤© Web ç•Œé¢ï¼Œæ”¯æŒæ‰€æœ‰é›†æˆçš„ LLM æä¾›å•†ã€‚

### å¯åŠ¨ Web ç•Œé¢

```bash
# åœ¨é¡¹ç›®ç›®å½•ä¸­è¿è¡Œ
streamlit run chat-webui.py
```

è®¿é—® `http://localhost:8501` å³å¯ä½¿ç”¨èŠå¤©ç•Œé¢ã€‚

### Web ç•Œé¢åŠŸèƒ½

- ğŸ¨ **ç°ä»£åŒ–UI**: ç¾è§‚çš„èŠå¤©ç•Œé¢ï¼Œæ”¯æŒæ·±è‰²/æµ…è‰²ä¸»é¢˜
- ğŸ”„ **å®æ—¶åˆ‡æ¢**: æ— éœ€é‡å¯å³å¯åˆ‡æ¢ä¸åŒçš„æ¨¡å‹å’Œæä¾›å•†
- ğŸ“Š **çŠ¶æ€ç›‘æ§**: å®æ—¶æ˜¾ç¤ºæœ¬åœ°æœåŠ¡å™¨ï¼ˆOllama/LM Studioï¼‰çŠ¶æ€
- ğŸ”§ **æ™ºèƒ½ç¼“å­˜**: ä¼˜åŒ–çš„æ¨¡å‹é…ç½®åŠ è½½ï¼Œé¿å…é‡å¤è¯·æ±‚
- ğŸ’¾ **å¯¹è¯å†å²**: ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒæ¸…ç©ºå†å²
- ğŸ› ï¸ **è°ƒè¯•ä¿¡æ¯**: è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè¿æ¥çŠ¶æ€æ˜¾ç¤º
- ğŸ” **æ¨¡å‹ç®¡ç†**: æŸ¥çœ‹å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼Œæ”¯æŒæœ¬åœ°å’Œäº‘ç«¯æ¨¡å‹

### ä½¿ç”¨æ­¥éª¤

1. **é€‰æ‹©æä¾›å•†**: ä»ä¾§è¾¹æ é€‰æ‹© LLM æä¾›å•†
2. **é€‰æ‹©æ¨¡å‹**: æ ¹æ®æä¾›å•†é€‰æ‹©å…·ä½“æ¨¡å‹
3. **è¿æ¥æµ‹è¯•**: ç‚¹å‡»"è¿æ¥æ¨¡å‹"è¿›è¡Œè¿æ¥æµ‹è¯•
4. **å¼€å§‹èŠå¤©**: åœ¨èŠå¤©æ¡†ä¸­è¾“å…¥æ¶ˆæ¯å¼€å§‹å¯¹è¯

### æœ¬åœ°æœåŠ¡å™¨æ”¯æŒ

- **Ollama**: è‡ªåŠ¨æ£€æµ‹æœåŠ¡çŠ¶æ€ï¼Œæ˜¾ç¤ºå¯ç”¨æ¨¡å‹åˆ—è¡¨
- **LM Studio**: ç›‘æ§æœåŠ¡å™¨çŠ¶æ€ï¼Œè·å–å·²åŠ è½½æ¨¡å‹
- **çŠ¶æ€æŒ‡ç¤º**: å®æ—¶æ˜¾ç¤ºæœåŠ¡å™¨è¿è¡ŒçŠ¶æ€å’Œè¿æ¥ä¿¡æ¯
- **ä¸€é”®åˆ·æ–°**: æ”¯æŒæ‰‹åŠ¨åˆ·æ–°æœåŠ¡å™¨çŠ¶æ€å’Œæ¨¡å‹åˆ—è¡¨

### ç¯å¢ƒé…ç½®æç¤º

Web ç•Œé¢æä¾›è¯¦ç»†çš„ç¯å¢ƒé…ç½®æŒ‡å¯¼ï¼š
- API å¯†é’¥è®¾ç½®è¯´æ˜
- æœ¬åœ°æœåŠ¡å™¨å¯åŠ¨æ–¹æ³•
- å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ
- è°ƒè¯•ä¿¡æ¯æ˜¾ç¤º

### ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹

```python
from llm_api import LLMClient

client = LLMClient()

# ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹
response = client.chat(
    "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
    model="llama3.1:latest",
    provider="Ollama"
)
print(response)
```

### ä½¿ç”¨LM Studioæœ¬åœ°æ¨¡å‹

```python
from llm_api import LLMClient

client = LLMClient()

# ä½¿ç”¨LM Studioæ¨¡å‹
response = client.chat(
    "è§£é‡Šé‡å­è®¡ç®—çš„åŸç†",
    model="llama-3.2-1b-instruct",  # æ›¿æ¢ä¸ºä½ åŠ è½½çš„æ¨¡å‹åç§°
    provider="LMStudio"
)
print(response)
```



### å…¼å®¹åŸæœ‰ä»£ç 

```python
# å…¼å®¹åŸæœ‰çš„call_llmå‡½æ•°
from llm_api.utils import call_llm
from pydantic import BaseModel

class MyResponse(BaseModel):
    answer: str
    confidence: float

result = call_llm(
    prompt="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    pydantic_model=MyResponse,
    model_name="gpt-4o",
    provider="OpenAI"
)
```

## é…ç½®

### ç¯å¢ƒå˜é‡

åˆ›å»º`.env`æ–‡ä»¶å¹¶è®¾ç½®ç›¸åº”çš„APIå¯†é’¥ï¼š

```env
# OpenAI
OPENAI_API_KEY=your_openai_api_key
OPENAI_API_BASE=https://api.openai.com/v1  # å¯é€‰

# Anthropic
ANTHROPIC_API_KEY=your_anthropic_api_key

# Google
GOOGLE_API_KEY=your_google_api_key

# Groq
GROQ_API_KEY=your_groq_api_key

# DeepSeek
DEEPSEEK_API_KEY=your_deepseek_api_key

# Ollamaï¼ˆæœ¬åœ°ï¼‰
OLLAMA_HOST=localhost  # å¯é€‰
OLLAMA_BASE_URL=http://localhost:11434  # å¯é€‰

# LM Studioï¼ˆæœ¬åœ°ï¼‰
LMSTUDIO_HOST=localhost  # å¯é€‰
LMSTUDIO_PORT=1234  # å¯é€‰
LMSTUDIO_BASE_URL=http://localhost:1234  # å¯é€‰


```

### æ¨¡å‹é…ç½®

æ¨¡å‹é…ç½®å­˜å‚¨åœ¨ `config/` ç›®å½•ä¸­ï¼š

- `api_models.json`: APIæ¨¡å‹é…ç½®
- `ollama_models.json`: Ollamaæ¨¡å‹é…ç½®
- `lmstudio_models.json`: LM Studioæ¨¡å‹é…ç½®

ä½ å¯ä»¥ä¿®æ”¹è¿™äº›é…ç½®æ–‡ä»¶æ¥æ·»åŠ æ–°æ¨¡å‹æˆ–è°ƒæ•´ç°æœ‰æ¨¡å‹è®¾ç½®ã€‚

## APIå‚è€ƒ

### LLMClientç±»

#### åˆå§‹åŒ–

```python
client = LLMClient(
    default_model="gpt-4o",  # é»˜è®¤æ¨¡å‹
    default_provider="OpenAI"  # é»˜è®¤æä¾›å•†
)
```

#### æ–¹æ³•

- `chat()`: åŸºæœ¬èŠå¤©æ¥å£
- `chat_with_structured_output()`: ç»“æ„åŒ–è¾“å‡ºæ¥å£
- `get_model()`: è·å–æ¨¡å‹å®ä¾‹
- `list_available_models()`: åˆ—å‡ºå¯ç”¨æ¨¡å‹
- `get_model_info()`: è·å–æ¨¡å‹ä¿¡æ¯
- `clear_cache()`: æ¸…é™¤æ¨¡å‹ç¼“å­˜

### å·¥å…·å‡½æ•°

```python
from llm_api.utils import chat, call_llm, get_model, list_models

# ç®€å•èŠå¤©
response = chat("ä½ å¥½")

# ç»“æ„åŒ–è¾“å‡ºï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰
result = call_llm(prompt, MyModel)

# è·å–æ¨¡å‹å®ä¾‹
model = get_model("gpt-4o", "OpenAI")

# åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
models = list_models()
```

## Ollamaæ”¯æŒ

LLM-APIæä¾›å®Œæ•´çš„Ollamaæ”¯æŒï¼ŒåŒ…æ‹¬ï¼š

- è‡ªåŠ¨æ£€æµ‹Ollamaå®‰è£…
- è‡ªåŠ¨å¯åŠ¨OllamaæœåŠ¡
- è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
- æ¨¡å‹ç®¡ç†ï¼ˆä¸‹è½½ã€åˆ é™¤ï¼‰
- Dockerç¯å¢ƒæ”¯æŒ





### LM Studioè®¾ç½®

1. **ä¸‹è½½å¹¶å®‰è£…LM Studio**: ä» [lmstudio.ai](https://lmstudio.ai) ä¸‹è½½
2. **åŠ è½½æ¨¡å‹**: åœ¨LM Studioä¸­ä¸‹è½½å¹¶åŠ è½½ä¸€ä¸ªæ¨¡å‹
3. **å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨**: åˆ‡æ¢åˆ°"Local Server"æ ‡ç­¾é¡µï¼Œç‚¹å‡»"Start Server"
4. **é…ç½®ç«¯å£**: é»˜è®¤è¿è¡Œåœ¨ `http://localhost:1234`

### LM Studioæ¨¡å‹ç®¡ç†

```python
from llm_api.lmstudio_utils import (
    ensure_lmstudio_server,
    list_lmstudio_models,
    get_lmstudio_info
)

# æ£€æŸ¥LM StudioæœåŠ¡å™¨çŠ¶æ€
if ensure_lmstudio_server():
    print("LM StudioæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")

# è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
models = list_lmstudio_models()
print(f"å¯ç”¨æ¨¡å‹: {models}")

# è·å–æœåŠ¡å™¨ä¿¡æ¯
info = get_lmstudio_info()
print(f"æœåŠ¡å™¨ä¿¡æ¯: {info}")
```

### Ollamaæ¨¡å‹ç®¡ç†

```python
from llm_api.ollama_utils import (
    ensure_ollama_and_model,
    get_locally_available_models,
    download_model
)

# ç¡®ä¿æ¨¡å‹å¯ç”¨ï¼ˆå¦‚æœä¸å­˜åœ¨ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
ensure_ollama_and_model("llama3.1:latest")

# è·å–æœ¬åœ°å¯ç”¨æ¨¡å‹
models = get_locally_available_models()

# æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
download_model("gemma3:4b")
```

## LM Studioæ”¯æŒ

LLM-APIæä¾›å®Œæ•´çš„LM Studioæ”¯æŒï¼ŒåŒ…æ‹¬ï¼š

- è‡ªåŠ¨æ£€æµ‹LM StudioæœåŠ¡å™¨çŠ¶æ€
- è·å–å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨
- æ¨¡å‹ä¿¡æ¯æŸ¥è¯¢
- å…¼å®¹OpenAI APIæ ¼å¼

### LM Studioè®¾ç½®

1. **ä¸‹è½½å¹¶å®‰è£…LM Studio**: ä» [lmstudio.ai](https://lmstudio.ai) ä¸‹è½½
2. **åŠ è½½æ¨¡å‹**: åœ¨LM Studioä¸­ä¸‹è½½å¹¶åŠ è½½ä¸€ä¸ªæ¨¡å‹
3. **å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨**: åˆ‡æ¢åˆ°"Local Server"æ ‡ç­¾ï¼Œç‚¹å‡»"Start Server"
4. **é…ç½®ç«¯å£**: é»˜è®¤è¿è¡Œåœ¨ `http://localhost:1234`

### LM Studioæ¨¡å‹ç®¡ç†

```python
from llm_api.lmstudio_utils import (
    ensure_lmstudio_server,
    list_lmstudio_models,
    get_lmstudio_info
)

# æ£€æŸ¥LM StudioæœåŠ¡å™¨çŠ¶æ€
if ensure_lmstudio_server():
    print("LM StudioæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")

# è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
models = list_lmstudio_models()
print(f"å¯ç”¨æ¨¡å‹: {models}")

# è·å–æœåŠ¡å™¨ä¿¡æ¯
info = get_lmstudio_info()
print(f"æœåŠ¡å™¨ä¿¡æ¯: {info}")
```

## é”™è¯¯å¤„ç†

LLM-APIæä¾›äº†å®Œæ•´çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼š

```python
from llm_api.exceptions import (
    LLMAPIError,
    ModelNotFoundError,
    APIKeyError,
    ModelProviderError,
    OllamaError,
    LMStudioError
)

try:
    response = client.chat("Hello")
except APIKeyError as e:
    print(f"APIå¯†é’¥é”™è¯¯: {e}")
except LMStudioError as e:
    print(f"LM Studioé”™è¯¯: {e}")
except OllamaError as e:
    print(f"Ollamaé”™è¯¯: {e}")
except LLMAPIError as e:
    print(f"APIå¯†é’¥é”™è¯¯: {e}")
except ModelNotFoundError as e:
    print(f"æ¨¡å‹æœªæ‰¾åˆ°: {e}")
except LLMAPIError as e:
    print(f"LLM APIé”™è¯¯: {e}")
```

## æœ€ä½³å®è·µ

1. **ç¯å¢ƒå˜é‡ç®¡ç†**: ä½¿ç”¨`.env`æ–‡ä»¶ç®¡ç†APIå¯†é’¥
2. **æ¨¡å‹é€‰æ‹©**: æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹
3. **é”™è¯¯å¤„ç†**: å§‹ç»ˆåŒ…å«é€‚å½“çš„é”™è¯¯å¤„ç†
4. **ç¼“å­˜åˆ©ç”¨**: é‡å¤ä½¿ç”¨åŒä¸€å®¢æˆ·ç«¯å®ä¾‹ä»¥åˆ©ç”¨ç¼“å­˜
5. **ç»“æ„åŒ–è¾“å‡º**: ä½¿ç”¨Pydanticæ¨¡å‹ç¡®ä¿è¾“å‡ºæ ¼å¼ä¸€è‡´

## å¼€å‘å·¥å…·

### é…ç½®éªŒè¯

ä½¿ç”¨é…ç½®éªŒè¯è„šæœ¬æ£€æŸ¥ç¯å¢ƒè®¾ç½®ï¼š

```bash
python validate_config.py
```

è¯¥è„šæœ¬ä¼šæ£€æŸ¥ï¼š
- ä¾èµ–åŒ…å®‰è£…çŠ¶æ€
- é…ç½®æ–‡ä»¶æ ¼å¼
- ç¯å¢ƒå˜é‡è®¾ç½®
- Ollamaå®‰è£…çŠ¶æ€
- åŸºæœ¬åŠŸèƒ½æµ‹è¯•

### æ€§èƒ½æµ‹è¯•

è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼š

```bash
python benchmark.py
```

æµ‹è¯•åŒ…æ‹¬ï¼š
- é¡ºåºè¯·æ±‚æµ‹è¯•
- å¹¶å‘è¯·æ±‚æµ‹è¯•
- ç»“æ„åŒ–è¾“å‡ºæµ‹è¯•
- å“åº”æ—¶é—´ç»Ÿè®¡

### å•å…ƒæµ‹è¯•

è¿è¡Œå®Œæ•´çš„å•å…ƒæµ‹è¯•å¥—ä»¶ï¼š

```bash
python test_llm_api.py
```

æˆ–ä½¿ç”¨pytestï¼š

```bash
pip install pytest
pytest test_llm_api.py -v
```

### åŒ…å®‰è£…

å®‰è£…ä¸ºPythonåŒ…ï¼š

```bash
pip install -e .
```

å®‰è£…åå¯ä»¥ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·ï¼š

```bash
llm-api-test  # è¿è¡Œæµ‹è¯•å¥—ä»¶
```

### æ€§èƒ½æµ‹è¯•

è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼š

```bash
python benchmark.py
```

æµ‹è¯•åŒ…æ‹¬ï¼š
- é¡ºåºè¯·æ±‚æµ‹è¯•
- å¹¶å‘è¯·æ±‚æµ‹è¯•
- ç»“æ„åŒ–è¾“å‡ºæµ‹è¯•
- å“åº”æ—¶é—´ç»Ÿè®¡

### å•å…ƒæµ‹è¯•

è¿è¡Œå®Œæ•´çš„å•å…ƒæµ‹è¯•å¥—ä»¶ï¼š

```bash
python test_llm_api.py
```



## é¡¹ç›®ç»“æ„

```
llm-api/
â”œâ”€â”€ __init__.py              # ä¸»å…¥å£æ–‡ä»¶
â”œâ”€â”€ client.py                # æ ¸å¿ƒLLMClientç±»
â”œâ”€â”€ models.py                # æ¨¡å‹å®šä¹‰å’Œé…ç½®
â”œâ”€â”€ exceptions.py            # è‡ªå®šä¹‰å¼‚å¸¸ç±»
â”œâ”€â”€ utils.py                 # å…¼å®¹æ€§å·¥å…·å‡½æ•°
â”œâ”€â”€ ollama_utils.py          # Ollamaç®¡ç†å·¥å…·
â”œâ”€â”€ lmstudio_utils.py        # LM Studioç®¡ç†å·¥å…·
â”œâ”€â”€ config_manager.py        # é…ç½®ç®¡ç†
â”œâ”€â”€ langchain_lmstudio.py    # LangChain LM Studioé€‚é…å™¨
â”œâ”€â”€ setup.py                 # åŒ…å®‰è£…é…ç½®
â”œâ”€â”€ requirements.txt         # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md               # é¡¹ç›®æ–‡æ¡£ï¼ˆä¸­æ–‡ï¼‰
â”œâ”€â”€ README_EN.md            # é¡¹ç›®æ–‡æ¡£ï¼ˆè‹±æ–‡ï¼‰
â”œâ”€â”€ example.py              # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ example_lmstudio.py     # LM Studioç¤ºä¾‹
â”œâ”€â”€ test_llm_api.py         # å•å…ƒæµ‹è¯•
â”œâ”€â”€ test_lmstudio.py        # LM Studioæµ‹è¯•
â”œâ”€â”€ validate_config.py      # é…ç½®éªŒè¯è„šæœ¬
â”œâ”€â”€ benchmark.py            # æ€§èƒ½æµ‹è¯•è„šæœ¬
â””â”€â”€ config/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ api_models.json     # APIæ¨¡å‹é…ç½®
    â”œâ”€â”€ ollama_models.json  # Ollamaæ¨¡å‹é…ç½®
    â””â”€â”€ lmstudio_models.json # LM Studioæ¨¡å‹é…ç½®
```

## è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)äº†è§£è¯¦æƒ…ã€‚

### å¼€å‘è®¾ç½®

1. Forkä»“åº“
2. å…‹éš†ä½ çš„fork: `git clone https://github.com/yourusername/llm-api.git`
3. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ: `python -m venv venv`
4. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ: `source venv/bin/activate` (Linux/Mac) æˆ– `venv\Scripts\activate` (Windows)
5. å®‰è£…å¼€å‘ä¾èµ–: `pip install -e ".[dev]"`
6. è¿è¡Œæµ‹è¯•: `pytest`

### ä»£ç é£æ ¼

æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥ä¿è¯ä»£ç è´¨é‡ï¼š

- **Black**: ä»£ç æ ¼å¼åŒ–
- **Flake8**: ä»£ç æ£€æŸ¥
- **MyPy**: ç±»å‹æ£€æŸ¥
- **Pytest**: æµ‹è¯•

è¿è¡Œæ‰€æœ‰æ£€æŸ¥ï¼š

```bash
black .
flake8 .
mypy .
pytest
```

## æ›´æ–°æ—¥å¿—

æŸ¥çœ‹[CHANGELOG.md](CHANGELOG.md)äº†è§£ç‰ˆæœ¬å†å²å’Œå˜æ›´ã€‚

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## æ”¯æŒ

å¦‚æœä½ é‡åˆ°ä»»ä½•é—®é¢˜æˆ–æœ‰ç–‘é—®ï¼š

1. æŸ¥çœ‹[æ–‡æ¡£](README.md)
2. æœç´¢[ç°æœ‰é—®é¢˜](https://github.com/ai-hedge-fund/llm-api/issues)
3. åˆ›å»º[æ–°é—®é¢˜](https://github.com/ai-hedge-fund/llm-api/issues/new)

## è‡´è°¢

- [LangChain](https://github.com/langchain-ai/langchain) æä¾›äº†ä¼˜ç§€çš„LLMæ¡†æ¶
- [Pydantic](https://github.com/pydantic/pydantic) æä¾›äº†æ•°æ®éªŒè¯
- [Ollama](https://ollama.ai/) æä¾›äº†æœ¬åœ°æ¨¡å‹æ”¯æŒ
- [LM Studio](https://lmstudio.ai/) æä¾›äº†æœ¬åœ°æ¨¡å‹ç®¡ç†
- æ‰€æœ‰LLMæä¾›å•†æä¾›äº†å‡ºè‰²çš„API