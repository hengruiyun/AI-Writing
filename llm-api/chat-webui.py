#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM API Chat Web UI
åŸºäºStreamlitçš„èŠå¤©Webç•Œé¢ï¼Œæ”¯æŒå¤šä¸ªLLMæä¾›å•†
"""

import streamlit as st
import sys
import os
from typing import List, Dict, Any
import traceback

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from client import LLMClient
    from models import ModelProvider, LLMModel
    from exceptions import LLMAPIError
    from config_manager import ConfigManager
    from ollama_utils import OllamaManager
    from lmstudio_utils import LMStudioManager
    from prompt_manager import get_prompt_manager, AgentRole
    from i18n import get_i18n, t, Language
except ImportError as e:
    st.error(f"Module import failed: {e}")
    st.error("Please ensure you are running this script in the llm-api project directory")
    st.stop()

# åˆå§‹åŒ–å›½é™…åŒ–
i18n = get_i18n()

# é¡µé¢é…ç½®
st.set_page_config(
    page_title=t("page_title"),
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
.main-header {
    font-size: 2.5rem;
    font-weight: bold;
    text-align: center;
    margin-bottom: 2rem;
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
}

.chat-message {
    padding: 1rem;
    border-radius: 0.5rem;
    margin-bottom: 1rem;
    border-left: 4px solid #667eea;
}

.user-message {
    background-color: #f0f2f6;
    border-left-color: #667eea;
}

.assistant-message {
    background-color: #e8f4fd;
    border-left-color: #1f77b4;
}

.error-message {
    background-color: #ffe6e6;
    border-left-color: #ff4444;
    color: #cc0000;
}

.success-message {
    background-color: #e6ffe6;
    border-left-color: #44ff44;
    color: #006600;
}

.sidebar .stSelectbox {
    margin-bottom: 1rem;
}
</style>
""", unsafe_allow_html=True)

@st.cache_data(ttl=30)  # ç¼“å­˜30ç§’
def get_local_server_info(provider: str) -> Dict[str, Any]:
    """è·å–æœ¬åœ°æœåŠ¡å™¨ä¿¡æ¯ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    if provider == "Ollama":
        try:
            ollama_manager = OllamaManager()
            is_running = ollama_manager.is_server_running()
            models = ollama_manager.get_locally_available_models() if is_running else []
            return {
                "status": "running" if is_running else "stopped",
                "url": ollama_manager.base_url,
                "models": models,
                "models_count": len(models),
                "error": None
            }
        except Exception as e:
            return {
                "status": "error",
                "url": "http://localhost:11434",
                "models": [],
                "models_count": 0,
                "error": str(e)
            }
    elif provider == "LMStudio":
        try:
            lmstudio_manager = LMStudioManager()
            is_running = lmstudio_manager.is_server_running()
            models = lmstudio_manager.get_model_names() if is_running else []
            return {
                "status": "running" if is_running else "stopped",
                "url": lmstudio_manager.base_url,
                "models": models,
                "models_count": len(models),
                "error": None
            }
        except Exception as e:
            return {
                "status": "error",
                "url": "http://localhost:1234",
                "models": [],
                "models_count": 0,
                "error": str(e)
            }
    else:
        return {
            "status": "error",
            "url": "N/A",
            "models": [],
            "models_count": 0,
            "error": t("unsupported_provider", provider=provider)
        }

def get_local_ollama_models() -> List[str]:
    """è·å–æœ¬åœ°Ollamaæ¨¡å‹åˆ—è¡¨"""
    server_info = get_local_server_info("Ollama")
    return server_info["models"]

def get_local_lmstudio_models() -> List[str]:
    """è·å–æœ¬åœ°LM Studioæ¨¡å‹åˆ—è¡¨"""
    server_info = get_local_server_info("LMStudio")
    return server_info["models"]

@st.cache_data(ttl=60)  # ç¼“å­˜60ç§’
def get_cloud_models() -> Dict[str, List[str]]:
    """è·å–äº‘ç«¯æ¨¡å‹åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        client = LLMClient()
        models = client.list_available_models()
        
        # æŒ‰æä¾›å•†åˆ†ç»„
        provider_models = {}
        for model in models:
            provider = model.provider.value
            if provider not in provider_models:
                provider_models[provider] = []
            provider_models[provider].append(model.model_name)
        
        return provider_models
    except Exception:
        # è¿”å›é»˜è®¤äº‘ç«¯æ¨¡å‹åˆ—è¡¨
        return {
            "OpenAI": ["gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo"],
            "Anthropic": ["claude-3-5-haiku-latest", "claude-3-5-sonnet-latest"],
            "Google": ["gemini-2.0-flash-exp", "gemini-1.5-pro"],
            "Groq": ["llama-3.3-70b-versatile", "mixtral-8x7b-32768"],
            "DeepSeek": ["deepseek-chat", "deepseek-reasoner"]
        }

@st.cache_data(ttl=120)  # ç¼“å­˜2åˆ†é’Ÿ
def get_available_models() -> Dict[str, List[str]]:
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼Œå¸¦ç¼“å­˜ï¼‰"""
    # è·å–äº‘ç«¯æ¨¡å‹
    provider_models = get_cloud_models().copy()
    
    # æ·»åŠ æœ¬åœ°æ¨¡å‹
    ollama_models = get_local_ollama_models()
    if ollama_models:
        provider_models["Ollama"] = ollama_models
    
    lmstudio_models = get_local_lmstudio_models()
    if lmstudio_models:
        provider_models["LMStudio"] = lmstudio_models
    
    return provider_models

def init_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "client" not in st.session_state:
        st.session_state.client = None
    
    if "connected" not in st.session_state:
        st.session_state.connected = False
    
    if "current_provider" not in st.session_state:
        st.session_state.current_provider = None
    
    if "current_model" not in st.session_state:
        st.session_state.current_model = None
    
    if "current_agent" not in st.session_state:
        st.session_state.current_agent = None
    
    if "prompt_manager" not in st.session_state:
        st.session_state.prompt_manager = get_prompt_manager()
    
    # åˆå§‹åŒ–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆé¿å…æ¯æ¬¡å¯¹è¯éƒ½é‡æ–°è·å–ï¼‰
    if "available_models" not in st.session_state:
        st.session_state.available_models = get_available_models()

def connect_to_model(provider: str, model: str) -> bool:
    """è¿æ¥åˆ°æŒ‡å®šçš„æ¨¡å‹"""
    try:
        client = LLMClient(default_model=model, default_provider=provider)
        
        # æµ‹è¯•è¿æ¥ - Ollamaä½¿ç”¨num_predictè€Œä¸æ˜¯max_tokens
        if provider == "Ollama":
            test_response = client.chat(
                "Hello", 
                model=model, 
                provider=provider,
                num_predict=10
            )
        else:
            test_response = client.chat(
                "Hello", 
                model=model, 
                provider=provider,
                max_tokens=10
            )
        
        st.session_state.client = client
        st.session_state.connected = True
        st.session_state.current_provider = provider
        st.session_state.current_model = model
        
        return True
    except Exception as e:
        st.error(t("connection_failed", error=str(e)))
        return False

def send_message(message: str) -> str:
    """å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤"""
    try:
        if not st.session_state.client:
            raise Exception(t("not_connected"))
        
        # å¦‚æœè®¾ç½®äº†æ™ºèƒ½ä½“ï¼Œä½¿ç”¨æ™ºèƒ½ä½“è¿›è¡ŒèŠå¤©
        if st.session_state.current_agent:
            response = st.session_state.client.chat(
                message,
                model=st.session_state.current_model,
                provider=st.session_state.current_provider,
                agent_id=st.session_state.current_agent
            )
        else:
            response = st.session_state.client.chat(
                message,
                model=st.session_state.current_model,
                provider=st.session_state.current_provider
            )
        
        return response
    except Exception as e:
        raise Exception(t("send_message_error", error=str(e)))

def test_local_server_connection(provider: str) -> Dict[str, Any]:
    """æµ‹è¯•æœ¬åœ°æœåŠ¡å™¨è¿æ¥çŠ¶æ€ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
    return get_local_server_info(provider)

def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    with st.sidebar:
        # è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿè¯­è¨€ï¼Œä¸æ˜¾ç¤ºè¯­è¨€é€‰æ‹©å™¨
        st.markdown("---")
        st.markdown(f"### ğŸ”§ {t('model_configuration')}")
        
        # æ¨¡å‹é…ç½®æ§åˆ¶
        col1, col2 = st.columns([3, 1])
        with col2:
            if st.button(f"ğŸ”„ {t('refresh_config')}", key="refresh_models", help=t('refresh_config')):
                # æ¸…é™¤æ‰€æœ‰ç›¸å…³ç¼“å­˜
                get_available_models.clear()
                get_cloud_models.clear()
                get_local_server_info.clear()
                # é‡æ–°è·å–å¯ç”¨æ¨¡å‹å¹¶æ›´æ–°session_state
                st.session_state.available_models = get_available_models()
                st.rerun()
        
        # ä½¿ç”¨session_stateä¸­çš„å¯ç”¨æ¨¡å‹ï¼ˆé¿å…æ¯æ¬¡å¯¹è¯éƒ½é‡æ–°åŠ è½½ï¼‰
        available_models = st.session_state.available_models
        
        # é€‰æ‹©æä¾›å•†
        provider = st.selectbox(
            t("select_provider"),
            options=list(available_models.keys()),
            key="provider_select"
        )
        
        # æ˜¾ç¤ºæœ¬åœ°æœåŠ¡å™¨çŠ¶æ€
        if provider in ["Ollama", "LMStudio"]:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.markdown(f"#### ğŸ“¡ {provider} {t('server_status')}")
            with col2:
                if st.button("ğŸ”„", key=f"refresh_{provider}"):
                    # æ¸…é™¤ç¼“å­˜å¹¶é‡æ–°è·å–
                    get_local_server_info.clear()
                    get_available_models.clear()
                    # é‡æ–°è·å–å¯ç”¨æ¨¡å‹å¹¶æ›´æ–°session_state
                    st.session_state.available_models = get_available_models()
                    st.rerun()
            
            server_info = test_local_server_connection(provider)
            
            # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
            st.info(f"**{t('server_url')}**: {server_info.get('url', 'N/A')}")
            
            if server_info["status"] == "running":
                st.success(f"ğŸŸ¢ {provider} {t('running')}")
                st.info(f"**{t('available_models')}**: {server_info['models_count']}")
                if server_info['models']:
                    with st.expander(t('available_models')):
                        for model in server_info['models']:
                            st.text(f"â€¢ {model}")
            elif server_info["status"] == "stopped":
                st.error(f"ğŸ”´ {provider} {t('stopped')}")
                st.warning(t('ensure_server_running', server=provider))
                # æ˜¾ç¤ºç¯å¢ƒå˜é‡æç¤º
                if provider == "Ollama":
                    st.info(f"ğŸ’¡ {t('environment_setup')} Ollama:")
                    st.code("OLLAMA_BASE_URL=http://your-host:port\nOLLAMA_HOST=your-host\nOLLAMA_PORT=your-port")
                elif provider == "LMStudio":
                    st.info(f"ğŸ’¡ {t('environment_setup')} LM Studio:")
                    st.code("LMSTUDIO_BASE_URL=http://your-host:port\nLMSTUDIO_HOST=your-host\nLMSTUDIO_PORT=your-port")
            else:
                st.error(f"âŒ {provider} {t('connection_error')}")
                st.error(f"{t('error_details')}: {server_info.get('error', 'Unknown error')}")
                # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                with st.expander(f"ğŸ” {t('debug_info')}"):
                    st.text(f"{t('server_url')}: {server_info.get('url', 'N/A')}")
                    st.text(f"{t('error_details')}: {server_info.get('error', 'Unknown error')}")
                    if provider == "Ollama":
                        st.text(t('please_check'))
                        st.text(t('ollama_check_1'))
                        st.text(t('ollama_check_2'))
                        st.text(t('ollama_check_3'))
                    elif provider == "LMStudio":
                        st.text(t('please_check'))
                        st.text(t('lmstudio_check_1'))
                        st.text(t('lmstudio_check_2'))
                        st.text(t('lmstudio_check_3'))
        
        # é€‰æ‹©æ¨¡å‹
        if provider and provider in available_models:
            models_list = available_models[provider]
            if models_list:
                model = st.selectbox(
                    t("select_model"),
                    options=models_list,
                    key="model_select"
                )
            else:
                model = None
                if provider in ["Ollama", "LMStudio"]:
                    st.warning(t("no_local_models_detected", provider=provider))
                else:
                    st.warning(t("no_models_available"))
        else:
            model = None
            st.warning(t("select_model_first"))
        
        # è¿æ¥æµ‹è¯•æŒ‰é’®ï¼ˆä»…æœ¬åœ°æœåŠ¡å™¨ï¼‰
        if provider in ["Ollama", "LMStudio"]:
            if st.button(f"ğŸ” {t('test_connection')} {provider}", use_container_width=True):
                with st.spinner(t("testing_connection")):
                    server_info = test_local_server_connection(provider)
                    if server_info["status"] == "running":
                        st.success(f"âœ… {provider} {t('test_successful')}")
                        st.balloons()
                    else:
                        st.error(f"âŒ {provider} {t('test_failed', error=server_info.get('error', 'Unknown error'))}")
                    st.rerun()
        
        # è¿æ¥æ¨¡å‹æŒ‰é’®
        if st.button(f"ğŸ”— {t('connect_model')}", type="primary", use_container_width=True):
            if provider and model:
                with st.spinner(t("connecting")):
                    if connect_to_model(provider, model):
                        st.success(f"âœ… {t('connection_successful')} {provider} - {model}")
                        st.rerun()
            else:
                st.error(t("select_model_first"))
        
        # æ™ºèƒ½ä½“ç®¡ç†
        st.markdown("---")
        st.markdown(f"### ğŸ¤– {t('agent_management')}")
        
        # è·å–å¯ç”¨çš„æ™ºèƒ½ä½“å’Œæ¨¡æ¿
        available_agents = st.session_state.prompt_manager.list_agents()
        available_templates = st.session_state.prompt_manager.list_templates()
        
        # åˆ›å»ºæ™ºèƒ½ä½“æ˜¾ç¤ºé€‰é¡¹ï¼ˆæ˜¾ç¤ºä¸­æ–‡åç§°ï¼‰
        def get_agent_display_name(agent_id):
            """è·å–æ™ºèƒ½ä½“çš„æ˜¾ç¤ºåç§°"""
            try:
                agent_config = st.session_state.prompt_manager.load_agent(agent_id)
                return agent_config.name if agent_config else agent_id
            except:
                return agent_id
        
        # æ™ºèƒ½ä½“é€‰æ‹©
        agent_display_options = [t("default_agent")] + [get_agent_display_name(agent) for agent in available_agents]
        agent_id_mapping = {t("default_agent"): None}
        for i, agent_id in enumerate(available_agents):
            display_name = get_agent_display_name(agent_id)
            agent_id_mapping[display_name] = agent_id
        
        # è·å–å½“å‰é€‰ä¸­çš„æ˜¾ç¤ºåç§°
        current_display_name = t("default_agent")
        if st.session_state.current_agent:
            current_display_name = get_agent_display_name(st.session_state.current_agent)
        
        selected_display_name = st.selectbox(
            t("select_agent"),
            options=agent_display_options,
            index=agent_display_options.index(current_display_name) if current_display_name in agent_display_options else 0,
            key="agent_select"
        )
        
        # è·å–å¯¹åº”çš„æ™ºèƒ½ä½“ID
        selected_agent = agent_id_mapping.get(selected_display_name)
        
        # æ›´æ–°å½“å‰æ™ºèƒ½ä½“
        if selected_agent is None:
            st.session_state.current_agent = None
        else:
            st.session_state.current_agent = selected_agent
            if st.session_state.client:
                st.session_state.client.set_agent(selected_agent)
        
        # æ˜¾ç¤ºå½“å‰æ™ºèƒ½ä½“ä¿¡æ¯
        if st.session_state.current_agent:
            agent_config = st.session_state.prompt_manager.load_agent(st.session_state.current_agent)
            if agent_config:
                st.info(f"**{t('agent_role')}**: {agent_config.role.value}")
                with st.expander(f"ğŸ“ {t('agent_prompt')}"):
                    st.text_area(
                        t("system_prompt"),
                        value=agent_config.system_prompt,
                        height=100,
                        disabled=True,
                        key="current_agent_prompt"
                    )
        
        # å¿«é€Ÿåˆ›å»ºæ™ºèƒ½ä½“
        with st.expander(f"â• {t('create_agent')}"):
            new_agent_name = st.text_input(t("agent_name"))
            selected_template = st.selectbox(
                t("select_template"),
                options=available_templates,
                key="template_select"
            )
            
            if st.button(f"âœ¨ {t('create_from_template')}", key="create_agent_btn"):
                if new_agent_name and selected_template:
                    agent_id = new_agent_name.lower().replace(" ", "_")
                    if st.session_state.client.create_agent_from_template(
                        agent_id=agent_id,
                        template_id=selected_template,
                        name=new_agent_name
                    ):
                        st.success(f"âœ… {t('agent_created')}: {new_agent_name}")
                        st.rerun()
                    else:
                        st.error(f"âŒ {t('agent_creation_failed')}")
                else:
                    st.error(t("fill_required_fields"))
        
        # è¿æ¥çŠ¶æ€
        st.markdown("---")
        st.markdown(f"### ğŸ“Š {t('connection_status')}")
        if st.session_state.connected:
            st.success(t("connected"))
            st.info(f"**{t('provider')}**: {st.session_state.current_provider}")
            st.info(f"**{t('model')}**: {st.session_state.current_model}")
            if st.session_state.current_agent:
                st.info(f"**{t('agent', default='æ™ºèƒ½ä½“')}**: {st.session_state.current_agent}")
            
            # æ–­å¼€è¿æ¥æŒ‰é’®
            if st.button(f"ğŸ”Œ {t('disconnect')}", use_container_width=True):
                st.session_state.client = None
                st.session_state.connected = False
                st.session_state.current_provider = None
                st.session_state.current_model = None
                st.session_state.current_agent = None
                st.rerun()
        else:
            st.error(t("not_connected"))
        
        # æ¸…ç©ºå¯¹è¯æŒ‰é’®
        st.markdown("---")
        if st.button(f"ğŸ—‘ï¸ {t('clear_conversation')}", use_container_width=True):
            st.session_state.messages = []
            st.success(t("conversation_cleared"))
            st.rerun()
        
        # å¸®åŠ©ä¿¡æ¯
        st.markdown("---")
        st.markdown(f"### ğŸ’¡ {t('usage_instructions')}")
        st.markdown(f"""
        {t('usage_step1')}
        {t('usage_step2')}
        {t('usage_step3')}
        """)
        
        # ç¯å¢ƒé…ç½®æç¤º
        st.markdown(f"### âš™ï¸ {t('environment_setup')}")
        
        # äº‘ç«¯APIé…ç½®
        with st.expander(f"â˜ï¸ {t('api_key_required')}"):
            st.markdown("""
            Please set the corresponding API keys:
            - `OPENAI_API_KEY`
            - `ANTHROPIC_API_KEY`
            - `GOOGLE_API_KEY`
            - `GROQ_API_KEY`
            - `DEEPSEEK_API_KEY`
            """)
        
        # æœ¬åœ°æœåŠ¡å™¨é…ç½®
        with st.expander(f"ğŸ  {t('local_server_setup')}"):
            st.markdown("""
            **Ollama Setup:**
            ```bash
            # Start Ollama service
            ollama serve
            
            # Download models (examples)
            ollama pull llama3.1
            ollama pull qwen2.5
            ```
            
            **LM Studio Setup:**
            1. Open LM Studio application
            2. Go to "Local Server" tab
            3. Select a model and click "Start Server"
            4. Default port: 1234
            
            **Local models do not require API keys**
            """)

def render_chat_interface():
    """æ¸²æŸ“èŠå¤©ç•Œé¢"""
    # ä¸»æ ‡é¢˜
    st.markdown(f'<h1 class="main-header">{t("main_header")}</h1>', unsafe_allow_html=True)
    
    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.markdown(
                f'<div class="chat-message user-message"><strong>ğŸ‘¤ {t("you")}:</strong><br>{message["content"]}</div>',
                unsafe_allow_html=True
            )
        else:
            st.markdown(
                f'<div class="chat-message assistant-message"><strong>ğŸ¤– {t("assistant")}:</strong><br>{message["content"]}</div>',
                unsafe_allow_html=True
            )
    
    # èŠå¤©è¾“å…¥
    if st.session_state.connected:
        # ä½¿ç”¨chat_inputç»„ä»¶
        if prompt := st.chat_input(t("chat_input_placeholder")):
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
            st.markdown(
                f'<div class="chat-message user-message"><strong>ğŸ‘¤ {t("you")}:</strong><br>{prompt}</div>',
                unsafe_allow_html=True
            )
            
            # è·å–AIå›å¤
            with st.spinner(f"ğŸ¤” {t('ai_thinking')}"):
                try:
                    response = send_message(prompt)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    
                    # æ˜¾ç¤ºAIå›å¤
                    st.markdown(
                        f'<div class="chat-message assistant-message"><strong>ğŸ¤– {t("assistant")}:</strong><br>{response}</div>',
                        unsafe_allow_html=True
                    )
                    
                except Exception as e:
                    error_msg = f"{t('error_label')}: {str(e)}"
                    st.markdown(
                        f'<div class="chat-message error-message"><strong>âŒ {t("error_label")}:</strong><br>{error_msg}</div>',
                        unsafe_allow_html=True
                    )
            
            # é‡æ–°è¿è¡Œä»¥æ›´æ–°ç•Œé¢
            st.rerun()
    else:
        st.info(f"ğŸ’¡ {t('please_select_model')}")
        st.chat_input(t("please_connect_model"), disabled=True)

def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    init_session_state()
    
    # æ¸²æŸ“ä¾§è¾¹æ 
    render_sidebar()
    
    # æ¸²æŸ“èŠå¤©ç•Œé¢
    render_chat_interface()
    
    # é¡µè„š
    st.markdown("---")
    st.markdown(
        f"<div style='text-align: center; color: #666; font-size: 0.8rem;'>"
        f"{t('powered_by')}"
        "</div>",
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()